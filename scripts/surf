#!/usr/bin/env python3

"""
surf: A standalone script to surf web (Powered by searx)

Copyright ¬© 2021 Bhupesh Varshney

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""

import urllib.request
import urllib.parse
import json
import pprint
import random
import argparse
import readline
from sys import stdin
from os import isatty

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36",
    "Content-Type": "application/x-www-form-urlencoded",
}


USAGE = """surf\n
(Assuimng surf is in your current working directory)
1. Just run the script, without any arguments:
   $ ./surf

2. Show link description:
   $ ./surf -s

3. Provide search query as parameter:
   $ ./surf -sq "how to change the world"

4. You can also pipe data to surf ¬Ø\_(„ÉÑ)_/¬Ø:
   $ echo "search this" | ./surf

5. Use -c to specify search category:
   $ surf -c "videos" -sq "how to make a pizza"
   Default category is "general". Other categories include: "news", "files", "images", "map"

   > For better experience, add surf to your PATH or move it to $HOME/.local/bin
   """


class colors:
    CYAN = "\033[1;36m"
    GREEN = "\033[40;38;5;154m"
    ORANGE_BG = "\033[30;48;5;214m"
    RESET = "\033[m"
    BOLD = "\033[;1m"


def request(url, data=None, method=None):
    req = urllib.request.Request(url, headers=HEADERS)
    # print(req.headers)
    try:
        with urllib.request.urlopen(req) as response:
            res = json.loads(response.read().decode("utf-8"))
    except urllib.error.URLError as e:
        return e.reason, e.code
    return res, response.code


def dont_hurt_instance(query, category):
    # searx is fighting to block bot traffic, we need to be nice
    # but we also need search results
    searx_instances = {
        "https://searx.nevrlands.de/",
        "https://searx.olymp.to/",
        "https://search.mdosch.de/",
        "https://dynabyte.ca/",
        "https://searx.fmac.xyz/",
        "https://searx.bar/",
        "https://searx.tuxcloud.net/",
        "https://metasearch.nl/",
        "https://searx.pwoss.org/",
        "https://privatesearch.app/",
        "https://procurx.pt/",
        "https://searx.devol.it/",
        "https://searx.ninja/",
    }
    overloaded_instance = set()
    while True:
        instance = random.choice(tuple(searx_instances))
        data, code = request(
            f"{instance}search?q={urllib.parse.quote_plus(query)}&categories={category.lower()}&format=json"
        )
        if code == 429:
            overloaded_instance.add(instance)
            print(f"Oh Snap! {instance} says {data} :(")
            searx_instances = searx_instances - overloaded_instance
            continue
        elif code == 200 and len(data["results"]) > 0:
            print(
                f"""Found {colors.BOLD}{len(data["results"])}{colors.RESET} results on {colors.BOLD}{instance}{colors.RESET}\n"""
            )
            break
    return data


def search(query, show_description, category):
    data = dont_hurt_instance(query, category)

    if len(data["corrections"]) > 0:
        print(f"""Did you mean {colors.BOLD}{data["corrections"]}{colors.RESET}?\n""")

    for res in data["results"]:
        print(f"""{colors.BOLD}{colors.ORANGE_BG} {res["title"]} {colors.RESET}""")
        if show_description and res["content"] != "":
            print(f"""\t{colors.GREEN}{res["content"]:4}{colors.RESET}""")
        print(
            f"{colors.BOLD}‚ñ∫ ",
            f"""{colors.CYAN}{res["url"]}{colors.RESET}""",
        )


if __name__ == "__main__":
    is_pipe = not isatty(stdin.fileno())
    parser = argparse.ArgumentParser(description="Surf Internet on Command line")
    parser.usage = USAGE
    parser.add_argument(
        "-q",
        "--query",
        type=str,
        help="Search query",
    )
    parser.add_argument(
        "-c",
        "--category",
        type=str,
        default="general",
        help="Search Category",
    )
    parser.add_argument(
        "-s",
        "--show-description",
        action="store_true",
        default=False,
        help="Show link description",
    )
    args = parser.parse_args()
    if is_pipe:
        query = stdin.read()
    elif not args.query:
        query = str(input("\nSearch üîçÔ∏è : "))
    else:
        query = args.query
    search(query, args.show_description, args.category)
