#!/usr/bin/env python3

"""
surf: A standalone script to surf web (Powered by searx)

Copyright ¬© 2021 Bhupesh Varshney

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""

import urllib.request
import urllib.parse
import json
import platform
import argparse
import readline
from random import choice
from pathlib import Path
from sys import stdin
from os import isatty

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36",
    "Content-Type": "application/json",
}

USAGE = """surf\n
(Assuming surf is in your current working directory)
   > For better experience, add surf to your PATH or move it to $HOME/.local/bin

1. Just run the script, without any arguments:
   $ ./surf

2. Show link description:
   $ ./surf -d

3. Provide search query as parameter:
   $ ./surf -dq "how to change the world"

4. You can also pipe data to surf ¬Ø\_(„ÉÑ)_/¬Ø:
   $ echo "search this" | ./surf

5. Use -c to specify search category (default: general):
   $ ./surf -c "videos" -sq "how to make a pizza"
   Other categories include: "news", "files", "images", "map"

6. Update instance cache:
   $ ./surf -udq "when is doomsday"

7. Show only top N results (--top):
   $ ./surf -t 5 -dq "check if key exists in map"
   """

# TODO: Use a priority based approach (account response time) while selecting a random instance #
# TODO: Add option to use only 1 user-specified instance (--searx) #
# TODO: Add option to show all instances in cache #
# TODO: Automatically append "go" if the instance supports results from google #


def read_cache(cache):
    return set(cache.read_text().splitlines())


def get_cache_path():
    # searx instances cache path
    user_home = Path.home()
    cache_name = ".searx_instances"
    os_name = platform.system()

    if os_name == "Linux":
        return user_home / ".cache" / cache_name
    if os_name == "Darwin":
        return user_home / "Library" / "Caches" / cache_name
    return user_home / "AppData" / "Local" / "Temp" / cache_name


class colors:
    """
    Define your fav colors here
    """

    CYAN = "\033[1;36m"
    GREEN = "\033[40;38;5;154m"
    # gray bg + orange fg
    TITLE_STYLE = "\033[;1m\033[38;5;220m\033[48;5;238m"
    RESET = "\033[m"
    BOLD = "\033[;1m"


def request(url):
    """Send a GET to url"""
    req = urllib.request.Request(url, headers=HEADERS)
    try:
        with urllib.request.urlopen(req) as response:
            res = json.loads(response.read().decode("utf-8"))
    except urllib.error.URLError as e:
        return e.reason, e.code
    except urllib.error.HTTPError as e:
        return e.reason, e.code
    return res, response.code


def update_instances_cache():
    """
    Check https://searx.space/ for exact count
    surf additionally ignores instances based on parameters like
    - HTTP Status Codes
    - Any reported Errors
    - Fishy Behaviour (e.g using analytics for user tracking)
    """
    print("Updating cache ...")
    data = request("https://searx.space/data/instances.json")
    instances = data[0]["instances"]
    with open(get_cache_path(), "w") as cache:
        for server in instances:
            if (
                instances[server]["http"]["status_code"] == 200
                and instances[server]["http"]["error"] is None
                and not instances[server]["comments"]
            ):
                cache.write(f"{server}\n")
    return read_cache(get_cache_path())


def get_instances():
    """
    read searx instances cache
    """
    instance_cache = get_cache_path()
    if instance_cache.is_file():
        return read_cache(instance_cache)
    print("Searx Instances cache not found!")
    return update_instances_cache()


def dont_hurt_instance(query, category):
    # searx is fighting to block bot traffic, we need to be nice
    # but we also need search results in case the user searches more frequently
    searx_instances = get_instances()
    overloaded_instance = set()
    while True:
        instance = choice(tuple(searx_instances))
        current_instance = urllib.parse.urlparse(instance).netloc
        data, code = request(
            f"""{instance}search?q={urllib.parse.quote_plus(query)}&categories={category.lower()}&format=json"""
        )
        if code == 429:
            overloaded_instance.add(instance)
            print(
                f"Oh Snap! {colors.BOLD}{current_instance}{colors.RESET} says {data} :("
            )
            searx_instances = searx_instances - overloaded_instance
            continue
        elif code == 200 and len(data["results"]) > 0:
            print(
                f"""Found {colors.BOLD}{len(data["results"])}{colors.RESET} results on {colors.BOLD}{current_instance}{colors.RESET}\n"""
            )
            break
    return data


def search(query, args):
    data = dont_hurt_instance(query, args.category)

    if len(data["corrections"]) == 2:
        print(
            f"""Did you mean {colors.BOLD}{data["corrections"][1]}{colors.RESET}?\n"""
        )

    if args.top is not None:
        top_results_count = args.top
        print(f"Showing only top {top_results_count} results\n")
    else:
        top_results_count = len(data["results"])

    for res in data["results"][:top_results_count]:
        print(f"""{colors.BOLD}{colors.TITLE_STYLE} {res["title"]} {colors.RESET}""")
        if args.show_description and "content" in res:
            print(f"""\t{colors.GREEN}{res["content"]}{colors.RESET}""")
        print(
            f"{colors.BOLD}‚ñ∫ ",
            f"""{colors.CYAN}{res["url"]}{colors.RESET}""",
        )


if __name__ == "__main__":
    is_pipe = not isatty(stdin.fileno())
    parser = argparse.ArgumentParser(description="Surf Internet on Command line")
    parser.usage = USAGE
    parser.epilog = """Help/Bug Report: varshneybhupesh@gmail.com"""
    parser.add_argument(
        "-q",
        "--query",
        type=str,
        help="Search query",
    )
    parser.add_argument(
        "-c",
        "--category",
        type=str,
        default="general",
        help="Search Category",
    )
    parser.add_argument(
        "-t",
        "--top",
        type=int,
        help="Show only top N results",
    )
    parser.add_argument(
        "-d",
        "--show-description",
        action="store_true",
        default=False,
        help="Show link description",
    )
    parser.add_argument(
        "-u",
        "--update-cache",
        action="store_true",
        default=False,
        help="Update searx instances cache list",
    )
    args = parser.parse_args()

    if args.update_cache:
        update_instances_cache()

    if is_pipe:
        query = stdin.read()
    elif not args.query:
        query = str(input("\nSearch üîçÔ∏è : "))
    else:
        query = args.query
    search(query, args)
