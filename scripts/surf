#!/usr/bin/env python3

"""
surf: A standalone script to surf web (Powered by searx)

Copyright ¬© 2021 Bhupesh Varshney

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""

import urllib.request
import urllib.parse
import json
import random
import argparse
import readline
from pathlib import Path
from sys import stdin
from os import isatty

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36",
    "Content-Type": "application/x-www-form-urlencoded",
}

instance_cache = Path.home() / ".searx_instances"

USAGE = """surf\n
(Assuimng surf is in your current working directory)
   > For better experience, add surf to your PATH or move it to $HOME/.local/bin

1. Just run the script, without any arguments:
   $ ./surf

2. Show link description:
   $ ./surf -s

3. Provide search query as parameter:
   $ ./surf -sq "how to change the world"

4. You can also pipe data to surf ¬Ø\_(„ÉÑ)_/¬Ø:
   $ echo "search this" | ./surf

5. Use -c to specify search category (default: general):
   $ surf -c "videos" -sq "how to make a pizza"
   Other categories include: "news", "files", "images", "map"

6. Update instance cache:
   $ surf -usq "when is doomsday"

Help/Bug Report: varshneybhupesh@gmail.com
   """

read_cache = lambda cache: set(cache.read_text().splitlines())


class colors:
    """
    Define your fav colors here
    """

    CYAN = "\033[1;36m"
    GREEN = "\033[40;38;5;154m"
    ORANGE_BG = "\033[30;48;5;214m"
    RESET = "\033[m"
    BOLD = "\033[;1m"


def request(url):
    # Send a GET to url
    req = urllib.request.Request(url, headers=HEADERS)
    try:
        response = urllib.request.urlopen(req)
        res = json.loads(response.read().decode("utf-8"))
    except urllib.error.URLError as e:
        return e.reason, e.code
    return res, response.code


def update_instances_cache():
    """
    Check https://searx.space/ for exact count
    surf ignores instances based on paramters like
    - HTTP Status Codes
    - Any reported Errors
    - Fishy Behaviour (e.g using analytics for user tracking)
    """
    print("Updating cache ...")
    data = request("https://searx.space/data/instances.json")
    instances = data[0]["instances"]
    with open(instance_cache, "w") as cache:
        for server in instances:
            if (
                instances[server]["http"]["status_code"] == 200
                and instances[server]["http"]["error"] is None
                and not instances[server]["comments"]
            ):
                cache.write(f"{i}\n")
    return read_cache(instance_cache)


def get_instances():
    # read searx instances cache
    if instance_cache.is_file():
        return read_cache(instance_cache)
    print("Searx Instances cache not found!")
    return update_instances_cache()


def dont_hurt_instance(query, category):
    # searx is fighting to block bot traffic, we need to be nice
    # but we also need search results in case the user searches more frequently
    searx_instances = get_instances()
    overloaded_instance = set()
    while True:
        instance = random.choice(tuple(searx_instances))
        data, code = request(
            f"{instance}search?q={urllib.parse.quote_plus(query)}&categories={category.lower()}&format=json"
        )
        if code == 429:
            overloaded_instance.add(instance)
            print(f"Oh Snap! {urllib.parse.urlparse(instance).netloc} says {data} :(")
            searx_instances = searx_instances - overloaded_instance
            continue
        if code == 200 and len(data["results"]) > 0:
            print(
                f"""Found {colors.BOLD}{len(data["results"])}{colors.RESET} results on {colors.BOLD}{instance}{colors.RESET}\n"""
            )
            break
    return data


def search(query, show_description, category):
    data = dont_hurt_instance(query, category)

    if len(data["corrections"]) > 0:
        print(f"""Did you mean {colors.BOLD}{data["corrections"]}{colors.RESET}?\n""")

    for res in data["results"]:
        print(f"""{colors.BOLD}{colors.ORANGE_BG} {res["title"]} {colors.RESET}""")
        if show_description and res["content"] != "":
            print(f"""\t{colors.GREEN}{res["content"]:4}{colors.RESET}""")
        print(
            f"{colors.BOLD}‚ñ∫ ",
            f"""{colors.CYAN}{res["url"]}{colors.RESET}""",
        )


if __name__ == "__main__":
    is_pipe = not isatty(stdin.fileno())
    parser = argparse.ArgumentParser(description="Surf Internet on Command line")
    parser.usage = USAGE
    parser.add_argument(
        "-q",
        "--query",
        type=str,
        help="Search query",
    )
    parser.add_argument(
        "-c",
        "--category",
        type=str,
        default="general",
        help="Search Category",
    )
    parser.add_argument(
        "-s",
        "--show-description",
        action="store_true",
        default=False,
        help="Show link description",
    )
    parser.add_argument(
        "-u",
        "--udpate-cache",
        action="store_true",
        default=False,
        help="Update searx instances cache",
    )
    args = parser.parse_args()

    if args.udpate_cache:
        update_instances_cache()

    if is_pipe:
        query = stdin.read()
    elif not args.query:
        query = str(input("\nSearch üîçÔ∏è : "))
    else:
        query = args.query
    search(query, args.show_description, args.category)
